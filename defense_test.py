"""
defense_test.py
Evaluate attack AUC after applying Four-Layer Hybrid Defence.
Assumes:
attack_model already pre-trained by membership_inference_attack.py 
defended_target.pth generated by defense_mechanism.py 
"""

import torch, torch.nn.functional as F, numpy as np
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
from membership_inference_attack import (
    extract_attack_features, evaluate_attack, AttackModel, Config as ATKCFG
)
from MIA_defense_mechanism import DefenceWrapper, CFG as DEF
from membership_inference_attack import SimpleCNN 

device = DEF.device
# 1) Read the defense target model
base = SimpleCNN().to(device) 
base.load_state_dict(torch.load("defended_target.pth", map_location=device))
target = DefenceWrapper(base).to(device).eval()

# 2) generate member / non-member features
print("Collecting features...")
trans = transforms.Compose([transforms.ToTensor(),
                            transforms.Normalize((0.5,), (0.5,))])
mnist_train = datasets.MNIST("./data", train=True, transform=trans)
mnist_test  = datasets.MNIST("./data", train=False, transform=trans)

member_idx  = list(range(0, 5000))
nonmem_idx  = list(range(5000))
member_ld   = DataLoader(Subset(mnist_train, member_idx), batch_size=ATKCFG.BATCH_ATTACK)
nonmem_ld   = DataLoader(Subset(mnist_test,  nonmem_idx), batch_size=ATKCFG.BATCH_ATTACK)

member_feats, nonmem_feats = [], []
with torch.no_grad():
    for xb, _ in member_ld:
        xb = xb.to(device)
        logits = target.forward(xb)          # defence works on logits
        member_feats.append(extract_attack_features(logits).cpu())
    for xb, _ in nonmem_ld:
        xb = xb.to(device)
        logits = target.forward(xb)
        nonmem_feats.append(extract_attack_features(logits).cpu())

member_feats = torch.cat(member_feats)
nonmem_feats = torch.cat(nonmem_feats)
min_len      = min(len(member_feats), len(nonmem_feats))
member_feats = member_feats[:min_len]
nonmem_feats = nonmem_feats[:min_len]

# label
y = torch.cat([torch.ones(min_len), torch.zeros(min_len)]).long()
X = torch.cat([member_feats, nonmem_feats])

# 3) Load attack best model (.pth)
attack = AttackModel(input_dim=X.shape[1]).to(device)
attack.load_state_dict(torch.load(f"{ATKCFG.LOG_DIR}/attack_model_best.pth", map_location=device))

# 4) Evaluation
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().fit(X)  #Attack viewpoint：re-calculate scaler
X_scaled = torch.tensor(scaler.transform(X)).float()
res = evaluate_attack(attack, X_scaled, y, scaler, title="Defended Target", already_scaled=True)
print("\n★ After defence, AUC = {:.4f}".format(res["auc"]))



def statistical_denoising_attack(model, input_data, num_queries=50):
    outputs = []
    for _ in range(num_queries):
        output = model(input_data)  # Multi-input insert at once
        outputs.append(output)
    
    # My noise setting is independent Gaussian noise, the attacker can remove the noise by averaging
    estimated_clean_output = torch.stack(outputs).mean(dim=0)
